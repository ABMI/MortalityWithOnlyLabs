search_criteria = list(strategy = "Cartesian"),
algorithm="gbm",
grid_id="depth_grid",
x = features,
y = target,
training_frame = h_train,
validation_frame = h_test,
ntrees = 10000,
learn_rate = 0.05,
learn_rate_annealing = 0.99,
sample_rate = 0.8,
col_sample_rate = 0.8,
seed = 12345,
stopping_rounds = 5,
stopping_tolerance = 1e-4,
stopping_metric = "AUC",
score_tree_interval = 10
)
grid
sortedGrid <- h2o.getGrid("depth_grid", sort_by="auc", decreasing = TRUE)
sortedGrid
topDepths = sortedGrid@summary_table$max_depth[1:5]
minDepth = min(as.numeric(topDepths))
maxDepth = max(as.numeric(topDepths))
gbm <- h2o.getModel(sortedGrid@model_ids[[1]])
print(h2o.auc(h2o.performance(gbm, newdata = h_test)))
gbm@parameters
model <- do.call(h2o.gbm,
## update parameters in place
{
p <- gbm@parameters
p$model_id = NULL          ## do not overwrite the original grid model
p$training_frame = h_train
p$validation_frame = h_test
p$nfolds = 5               ## cross-validation
p
}
)
model@model$cross_validation_metrics_summary
for (i in 1:5) {
gbm <- h2o.getModel(sortedGrid@model_ids[[i]])
cvgbm <- do.call(h2o.gbm,
## update parameters in place
{
p <- gbm@parameters
p$model_id = NULL          ## do not overwrite the original grid model
p$training_frame = h_train      ## use the full dataset
p$validation_frame = h_test  ## no validation frame
p$nfolds = 5               ## cross-validation
p
}
)
print(gbm@model_id)
print(cvgbm@model$cross_validation_metrics_summary[5,]) ## Pick out the "AUC" row
}
model_gbm <- h2o.getModel(sortedGrid@model_ids[[1]])
#################################################################
#########################Tuning for DNN ########################################
hyper_params <- list(
activation=c("Rectifier","Tanh","Maxout","RectifierWithDropout","TanhWithDropout","MaxoutWithDropout"),
hidden=list(c(20,20),c(50,50),c(30,30,30),c(25,25,25,25)),
input_dropout_ratio=c(0,0.05),
l1=seq(0,1e-4,1e-6),
l2=seq(0,1e-4,1e-6)
)
hyper_params
## Stop once the top 5 models are within 1% of each other (i.e., the windowed average varies less than 1%)
search_criteria = list(strategy = "RandomDiscrete", max_runtime_secs = 360, max_models = 100, seed=1234567, stopping_rounds=5, stopping_tolerance=1e-2)
dl_random_grid <- h2o.grid(
algorithm="deeplearning",
grid_id = "dl_grid_random",
training_frame=h_train,
validation_frame=h_test,
x=features,
y=target,
epochs=1,
stopping_metric="logloss",
stopping_tolerance=1e-2,        ## stop when logloss does not improve by >=1% for 2 scoring events
stopping_rounds=2,
score_validation_samples=10000, ## downsample validation set for faster scoring
score_duty_cycle=0.025,         ## don't score more than 2.5% of the wall time
max_w2=10,                      ## can help improve stability for Rectifier
hyper_params = hyper_params,
search_criteria = search_criteria
)
grid <- h2o.getGrid("dl_grid_random",sort_by="logloss",decreasing=FALSE)
grid
grid@summary_table[1,]
model_dnn <- h2o.getModel(grid@model_ids[[1]]) ## model with lowest logloss
#################################################################
#########################Tuning for XGB ########################################
hyper_params = list( max_depth = seq(1,29,2) )
grid <- h2o.grid(
hyper_params = hyper_params,
search_criteria = list(strategy = "Cartesian"),
algorithm="xgboost",
grid_id="depth_grid",
x = features,
y = target,
training_frame = h_train,
validation_frame = h_test,
ntrees = 10000,
learn_rate = 0.05,
sample_rate = 0.8,
col_sample_rate = 0.8,
seed = 12345,
stopping_rounds = 5,
stopping_tolerance = 1e-4,
stopping_metric = "AUC",
score_tree_interval = 10
)
grid
sortedGrid <- h2o.getGrid("depth_grid", sort_by="auc", decreasing = TRUE)
sortedGrid
topDepths = sortedGrid@summary_table$max_depth[1:5]
minDepth = min(as.numeric(topDepths))
maxDepth = max(as.numeric(topDepths))
xgboost <- h2o.getModel(sortedGrid@model_ids[[1]])
print(h2o.auc(h2o.performance(xgboost, newdata = h_test)))
xgboost@parameters
model <- do.call(h2o.xgboost,
## update parameters in place
{
p <- xgboost@parameters
p$model_id = NULL          ## do not overwrite the original grid model
p$training_frame = df      ## use the full dataset
p$validation_frame = NULL  ## no validation frame
p$nfolds = 5               ## cross-validation
p
}
)
model@model$cross_validation_metrics_summary
for (i in 1:5) {
xgboost <- h2o.getModel(sortedGrid@model_ids[[i]])
cvgbm <- do.call(h2o.xgboost,
## update parameters in place
{
p <- xgboost@parameters
p$model_id = NULL          ## do not overwrite the original grid model
p$training_frame = h_train      ## use the full dataset
p$validation_frame = h_test  ## no validation frame
p$nfolds = 5               ## cross-validation
p
}
)
print(xgboost@model_id)
print(cvgbm@model$cross_validation_metrics_summary[5,]) ## Pick out the "AUC" row
}
model_xgb <- h2o.getModel(sortedGrid@model_ids[[1]])
#################################################################
automl = h2o.automl(x = features,
y = target,
training_frame = h_train,
validation_frame = h_test,
nfolds = 5,                     # 5-fold Cross-Validation
max_models = 30,                # Max number of models
stopping_metric = "AUC",       # Metric to optimize
seed = n_seed)
h2o.performance(model_dnn)
h2o.auc(h2o.performance(model_dnn))
h2o.auc(h2o.performance(model_dnn), train = TRUE, valid = TRUE)
h2o.auc(h2o.performance(model_gbm), train = TRUE, valid = TRUE)
h2o.auc(h2o.performance(model_glm), train = TRUE, valid = TRUE)
h2o.auc(h2o.performance(model_xgb), train = TRUE, valid = TRUE)
h2o.auc(h2o.performance(model_automl), train = TRUE, valid = TRUE)
h2o.auc(h2o.performance(automl), train = TRUE, valid = TRUE)
automl
h2o.shap_explain_row_plot(model_xgb)
h2o.shap_explain_row_plot(model_xgb, newdata = h_test)
a <- h2o.shap_explain_row_plot(model_xgb, newdata = h_test)
a
h2o.shap_summary_plot(model_xgb, newdata = h_test)
shap_explain_row_plot <- h2o.shap_explain_row_plot(model_xgb, h_test, row_index = 1)
shap_explain_row_plot
shap_explain_row_plot <- h2o.shap_explain_row_plot(model_xgb, h_test, row_index = 2)
print(shap_explain_row_plot)
shap_explain_row_plot <- h2o.shap_explain_row_plot(model_xgb, h_test)
print(shap_explain_row_plot)
shap_explain_row_plot <- h2o.shap_explain_row_plot(model_xgb, h_test, row_index = 3)
print(shap_explain_row_plot)
h2o.shap_summary_plot(model_xgb, newdata = h_test)
h2o.shap_summary_plot(model_xgb, newdata = h_train)
gbm@parameters
model <- do.call(h2o.gbm,
## update parameters in place
{
p <- gbm@parameters
p$model_id = NULL          ## do not overwrite the original grid model
p$training_frame = h_train
p$validation_frame = h_test
p$nfolds = 5               ## cross-validation
p
}
)
model@model$cross_validation_metrics_summary
for (i in 1:5) {
gbm <- h2o.getModel(sortedGrid@model_ids[[i]])
cvgbm <- do.call(h2o.gbm,
## update parameters in place
{
p <- gbm@parameters
p$model_id = NULL          ## do not overwrite the original grid model
p$training_frame = h_train      ## use the full dataset
p$validation_frame = h_test  ## no validation frame
p$nfolds = 5               ## cross-validation
p
}
)
print(gbm@model_id)
print(cvgbm@model$cross_validation_metrics_summary[5,]) ## Pick out the "AUC" row
}
sortedGrid@model_ids
sortedGrid@model_ids[[1]]
model_gbm
h2o.auc(model_dnn, train = T, valid = T, xval = FALSE)
h2o.auc(model_dnn, train = T, valid = T, xval = FALSE)
h2o.auc(model_gbm, train = T, valid = T, xval = FALSE)
h2o.auc(model_xgb, train = T, valid = T, xval = FALSE)
h2o.auc(model_glm, train = T, valid = T, xval = FALSE)
h2o.auc(model_glm, train = T, valid = T, xval = FALSE)
h2o.shap_summary_plot(model_xgb, newdata = h_train)
h2o.shap_summary_plot(model_xgb, newdata = h_test)
h2o.shap_summary_plot(model_dnn, newdata = h_test)
h2o.shap_summary_plot(model_gbm, newdata = h_test)
h2o.shap_summary_plot(model_xgb, newdata = h_test)
h2o.shap_summary_plot(model_glm, newdata = h_test)
model_glm <- h2o.glm(x = features,               # All 19 features
y = target,                 # In-hospital mortality
training_frame = h_train,   # H2O dataframe with training data
validation_frame = h_test, # H2O dataframe with test data
model_id = "tuned_glm",  # Give the model a name
nfolds = 5,                 # Using 5-fold CV
seed = n_seed)
h2o.auc(h2o.performance(model_glm), train = TRUE, valid = TRUE)
h2o.auc(h2o.performance(model_xgb), train = TRUE, valid = TRUE)
h2o.auc(model_glm, train = T, valid = T, xval = FALSE)
h2o.aucpr(model_dnn, train = T, valid = T, xval = FALSE)
h2o.auc(model_dnn, train = T, valid = T, xval = FALSE)
h2o.auc(model_gbm, train = T, valid = T, xval = FALSE)
h2o.auc(model_xgb, train = T, valid = T, xval = FALSE)
h2o.auc(model_glm, train = T, valid = T, xval = FALSE)
gbm@parameters
h2o.getModel(sortedGrid@model_ids[[1]])
grid <- h2o.grid(
hyper_params = hyper_params,
search_criteria = list(strategy = "Cartesian"),
algorithm="gbm",
grid_id="depth_grid",
x = features,
y = target,
training_frame = h_train,
validation_frame = h_test,
ntrees = 10000,
learn_rate = 0.05,
learn_rate_annealing = 0.99,
sample_rate = 0.8,
col_sample_rate = 0.8,
seed = 12345,
stopping_rounds = 5,
stopping_tolerance = 1e-4,
stopping_metric = "AUC",
score_tree_interval = 10
)
hyper_params
#########################Tuning for GBM ########################################
hyper_params = list( max_depth = seq(1,29,2) )
grid <- h2o.grid(
hyper_params = hyper_params,
search_criteria = list(strategy = "Cartesian"),
algorithm="gbm",
grid_id="depth_grid",
x = features,
y = target,
training_frame = h_train,
validation_frame = h_test,
ntrees = 10000,
learn_rate = 0.05,
learn_rate_annealing = 0.99,
sample_rate = 0.8,
col_sample_rate = 0.8,
seed = 12345,
stopping_rounds = 5,
stopping_tolerance = 1e-4,
stopping_metric = "AUC",
score_tree_interval = 10
)
grid
sortedGrid <- h2o.getGrid("depth_grid", sort_by="auc", decreasing = TRUE)
sortedGrid
topDepths = sortedGrid@summary_table$max_depth[1:5]
minDepth = min(as.numeric(topDepths))
maxDepth = max(as.numeric(topDepths))
topDepths
minDepth
maxDepth
gbm <- h2o.getModel(sortedGrid@model_ids[[1]])
gbm
print(h2o.auc(h2o.performance(gbm, newdata = h_test)))
gbm@parameters
model <- do.call(h2o.gbm,
## update parameters in place
{
p <- gbm@parameters
p$model_id = NULL          ## do not overwrite the original grid model
p$training_frame = h_train
p$validation_frame = h_test
p$nfolds = 5               ## cross-validation
p
}
)
traceback()
model <- do.call(h2o.gbm,
## update parameters in place
{
p <- gbm@parameters
p$model_id = NULL          ## do not overwrite the original grid model
p$training_frame = h_train
#p$validation_frame = h_test
p$nfolds = 5               ## cross-validation
p
}
)
for (i in 1:5) {
gbm <- h2o.getModel(sortedGrid@model_ids[[i]])
cvgbm <- do.call(h2o.gbm,
## update parameters in place
{
p <- gbm@parameters
p$model_id = NULL          ## do not overwrite the original grid model
p$training_frame = h_train      ## use the full dataset
p$validation_frame = h_test  ## no validation frame
p$nfolds = 5               ## cross-validation
p
}
)
print(gbm@model_id)
print(cvgbm@model$cross_validation_metrics_summary[5,]) ## Pick out the "AUC" row
}
model_gbm <- h2o.getModel(sortedGrid@model_ids[[1]])
model_gbm
gbm@parameters
model <- do.call(h2o.gbm,
## update parameters in place
{
p <- gbm@parameters
p$model_id = gbm          ## do not overwrite the original grid model
p$training_frame = h_train
p$validation_frame = h_test
p$nfolds = 5               ## cross-validation
p
}
)
gbm@parameters
model <- do.call(h2o.gbm,
## update parameters in place
{
p <- gbm@parameters
p$model_id = NULL          ## do not overwrite the original grid model
p$training_frame = h_train
p$validation_frame = h_test
p$nfolds = 5               ## cross-validation
p
}
)
#########################Tuning for RF ########################################
hyper_grid.h2o <- list(ntrees = seq(50, 500, by = 50),
mtries = seq(3, 5, by = 1),
#max_depth = seq(10, 30, by = 10),
#min_rows = seq(1, 3, by = 1),
#nbins = seq(20, 30, by = 10),
sample_rate = c(0.55, 0.632, 0.75))
# The number of models is 90:
sapply(hyper_grid.h2o, length) %>% prod()
system.time(grid_cartesian <- h2o.grid(algorithm = "randomForest",
grid_id = "rf_grid1",
x = features,
y = target,
seed = 29,
nfolds = 10,
training_frame = h_train,
validation_frame = h_test,
stopping_metric = "AUC",
hyper_params = hyper_grid.h2o,
search_criteria = list(strategy = "Cartesian")))
# Collect the results and sort by our model performance metric of choice:
grid_perf <- h2o.getGrid(grid_id = "rf_grid1",
sort_by = "auc",
decreasing = FALSE)
# Best model chosen by validation error:
model_drf <- h2o.getModel(grid_perf@model_ids[[1]])
model_drf
h2o.auc(h2o.performance(model_dnn), train = TRUE, valid = TRUE)
h2o.auc(h2o.performance(model_gbm), train = TRUE, valid = TRUE)
h2o.auc(h2o.performance(model_glm), train = TRUE, valid = TRUE)
h2o.auc(h2o.performance(model_xgb), train = TRUE, valid = TRUE)
h2o.auc(h2o.performance(model_drf), train = TRUE, valid = TRUE)
h2o.auc(model_dnn, train = T, valid = T, xval = FALSE)
h2o.auc(model_gbm, train = T, valid = T, xval = FALSE)
h2o.auc(model_xgb, train = T, valid = T, xval = FALSE)
h2o.auc(model_glm, train = T, valid = T, xval = FALSE)
h2o.auc(model_drf, train = T, valid = T, xval = FALSE)
h2o.aucpr(model_dnn, train = T, valid = T, xval = FALSE)
h2o.aucpr(model_gbm, train = T, valid = T, xval = FALSE)
h2o.aucpr(model_xgb, train = T, valid = T, xval = FALSE)
h2o.aucpr(model_glm, train = T, valid = T, xval = FALSE)
h2o.aucpr(model_drf, train = T, valid = T, xval = FALSE)
h2o.auc(automl, train = T, valid = T, xval = FALSE)
automl
automl@leader
h2o.auc(automl@leader, train = T, valid = T, xval = FALSE)
h2o.aucpr(automl@leader, train = T, valid = T, xval = FALSE)
h2o.shap_summary_plot(automl@leader, newdata = h_test)
install.packages("RtutoR")
# 0. load the data
df2 <- read.csv("~/temp/VitalDB_20200508_3daysLab_average_death (1).csv")
res = generate_exploratory_analysis_ppt(df="df2",
target_var = "Albumin",
top_k_features = 5,
f_screen_model = "information.gain",
group_name = "Sex")
res = RtutoR::gen_exploratory_report_app(df="df2",
target_var = "Albumin",
top_k_features = 5,
f_screen_model = "information.gain",
group_name = "Sex")
RtutoR::gen_exploratory_report_app(df="df2",
target_var = "Albumin",
top_k_features = 5,
f_screen_model = "information.gain",
group_name = "Sex")
RtutoR::gen_exploratory_report_app(df2)
h2o.auc(model_dnn, train = T, valid = T, xval = FALSE)
h2o.auc(model_gbm, train = T, valid = T, xval = FALSE)
h2o.auc(model_xgb, train = T, valid = T, xval = FALSE)
h2o.auc(model_glm, train = T, valid = T, xval = FALSE)
h2o.auc(model_drf, train = T, valid = T, xval = FALSE)
h2o.auc(automl@leader, train = T, valid = T, xval = FALSE)
h2o.aucpr(model_dnn, train = T, valid = T, xval = FALSE)
h2o.aucpr(model_gbm, train = T, valid = T, xval = FALSE)
h2o.aucpr(model_xgb, train = T, valid = T, xval = FALSE)
h2o.aucpr(model_glm, train = T, valid = T, xval = FALSE)
h2o.aucpr(model_drf, train = T, valid = T, xval = FALSE)
h2o.aucpr(automl@leader, train = T, valid = T, xval = FALSE)
automl@leader
automl@leaderboard
pred <- function(model, newdata)  {
results <- as.data.frame(h2o.predict(model,
newdata %>% as.h2o()))
return(results[[3L]])
}
explainer_glm <- explain(
model            = model_glm,
type             = "classification",
data             = features,
y                = target,
predict_function = pred,
label            = "h2o_glm"
)
pred <- function(model, newdata)  {
results <- as.data.frame(h2o.predict(model,
newdata %>% as.h2o()))
return(results[[3L]])
}
explainer_glm <- explain(
model            = model_glm,
type             = "classification",
data             = features,
y                = target,
predict_function = pred,
label            = "h2o_glm"
)
library(dplyr)
x_valid <-
h_test %>% select(-death_inhosp) %>% as_tibble()
# change response variable to a numeric binary vector
y_valid <-
as.vector(as.numeric(as.character(h_test$death_inhosp)))
as.data.frame(h_test)
h_test
data.frame(h_test)
explainer_glm <- DALEX::explain(model = model_glm,
data = as.data.frame(h_test)[, features],
y = as.data.frame(h_test)[, target],
predict_function = custom_predict,
label = "Generalized Linear Model")
h2o.init()
explainer_glm <- DALEX::explain(model = model_glm,
data = as.data.frame(h_test)[, features],
y = as.data.frame(h_test)[, target],
predict_function = custom_predict,
label = "Generalized Linear Model")
explainer_glm
model_glm
explainer_glm <- DALEX::explain(model = model_glm,
data = as.data.frame(h_test)[, features],
y = as.data.frame(h_test)[, target],
predict_function = custom_predict,
label = "Generalized Linear Model")
explainer_glm <- DALEX::explain(model = model_glm,
data = as.data.frame(h_test)[, features],
type = "classification",
y = as.data.frame(h_test)[, target],
predict_function = custom_predict,
label = "Generalized Linear Model")
h2o.rm()
explainer_glm <- DALEX::explain(model = model_glm,
data = as.data.frame(h_test)[, features],
type = "classification",
y = as.data.frame(h_test)[, target],
predict_function = custom_predict,
label = "Generalized Linear Model")
custom_predict
