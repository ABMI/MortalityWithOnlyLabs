#VitalDB model

# 0. load the data
df <- read.csv("~/temp/VitalDB_20200508_3daysLab_average_death (1).csv")

# 1. preparation for modeling
library(glmnet)

set.seed(1234)

label = as.integer(df$death_inhosp)
df$death_inhosp = NULL
df$caseid = NULL

n <- nrow(df)

train.index = sample(n,floor(0.75*n))
train.data = as.matrix(df[train.index,])
train.label = label[train.index]
test.data = as.matrix(df[-train.index,])
test.label = label[-train.index]

# 2. L1 regression from glmnet 
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)
# Setting alpha = 1 implements lasso regression
lasso_cv <- cv.glmnet(train.data, 
                      train.label, 
                      family="binomial",
                      alpha = 1, 
                      lambda = lambdas_to_try,
                      type.measure = "auc",
                      standardize = TRUE, nfolds = 3)
plot(lasso_cv)

lambda_cv <- lasso_cv$lambda.min
# Fit final model, get its sum of squared residuals and multiple R-squared
model_cv <- glmnet(train.data, train.label, alpha = 1, lambda = lambda_cv, standardize = TRUE, family = "binomial")

#lasso_assess <- assess.glmnet2(model_cv, newx = test.data, newy = test.label, family="binomial", type.measure = "auc")

pred <- predict(model_cv, s = lambda_cv, newx = train.data, type="response")

train_lasso_matrix <- confusion.glmnet(model_cv, newx = train.data, newy = train.label)
train_lasso_auc <- roc.glmnet(model_cv, newx = train.data, newy = train.label)

plot(train_lasso_auc)

pred <- predict(model_cv, s = lambda_cv, newx = test.data, type="response")
pROC::auc(test.label,pred)
test_lasso_matrix <- confusion.glmnet(model_cv, newx = test.data, newy = test.label)
test_lasso_auc <- roc.glmnet(model_cv, newx = test.data, newy = test.label)

plot(test_lasso_auc)

# 3. xgboost
library(xgboost)

xgb.train = xgb.DMatrix(data=train.data,label=train.label)
xgb.test = xgb.DMatrix(data=test.data,label=test.label)

params = list(
  booster="gbtree",
  eta=0.001,
  max_depth=5,
  gamma=3,
  subsample=0.75,
  colsample_bytree=1,
  objective="binary:logistic",
  eval_metric="auc"
)

xgbcv <- xgb.cv(params = params, 
                data = xgb.train, 
                nrounds = 100, 
                nfold = 3, 
                showsd = T, 
                stratified = T, 
                print.every.n = 10, 
                early.stop.round = 20, 
                maximize = F,
                standardized = T)

xgb.fit=xgb.train(
  params=params,
  data=xgb.train,
  nrounds=xgbcv$best_iteration,
  prediction=T
)

impMatrix <- xgb.importance(feature_names = dimnames(train.data)[[2]], model = xgb.fit)
xgb.plot.importance(impMatrix, main = "Gain by Feature")

library(InformationValue)

pred <- predict(xgb.fit, train.data)
opt.value <- optimalCutoff(train.label, pred)

pred <- predict(xgb.fit, test.data)
y <- ifelse(pred > 0.5, 1, 0)

xgb_matrix <- confusionMatrix(y, pred, threshold = opt.value)
xgb_auc <- xgb_auc(y, pred)


pROC::auc(train.label,pred)
